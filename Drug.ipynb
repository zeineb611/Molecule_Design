{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e5c0a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model selection and evaluation\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#import cv2\n",
    "import gc\n",
    "import os\n",
    " \n",
    "import tensorflow as tf\n",
    "\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import rdkit\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.layers import   Dense,Flatten, Reshape, LeakyReLU, Dropout,InputLayer,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from rdkit import Chem\n",
    "sns.set_style(style=\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fff0c724-752c-4b18-8b79-602124c44004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikeras in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (0.4.1)\n",
      "Requirement already satisfied: packaging<22.0,>=0.21 in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (from scikeras) (21.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (from scikeras) (1.0.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (from packaging<22.0,>=0.21->scikeras) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->scikeras) (1.22.4)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->scikeras) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->scikeras) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\zeineb\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->scikeras) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikeras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb2d9168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus: \n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df42bfdb",
   "metadata": {},
   "source": [
    "# DRUG DISCOVERY PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386e356e",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 128. KiB for an array with shape (16384,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_bad_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m;\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1253\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1251\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1253\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1254\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1172\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1731\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 128. KiB for an array with shape (16384,) and data type int64"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", error_bad_lines=False,sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb30c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df076b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51da54",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cefe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39eea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.replace('None',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6be56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.replace('',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840d848",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32b422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_values_percentage_by_column(df):\n",
    "    cols = df.columns.tolist()\n",
    "    missing_percentage = {}\n",
    "    for col in cols:\n",
    "        total_cells = df[col].size\n",
    "        missing_cells = df[col].isnull().sum()\n",
    "        percentage = (missing_cells / total_cells) * 100\n",
    "        missing_percentage[col] = percentage\n",
    "    return missing_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_percentage_by_column(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff6536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_missing_values(df):\n",
    "    missing_df = pd.DataFrame.from_dict(missing_values_percentage_by_column(data), orient='index', columns=['percentage'])\n",
    "    missing_df = missing_df.sort_values(by='percentage', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(missing_df.index, missing_df['percentage'])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('% of Missing Values')\n",
    "    plt.title('Missing Value Distribution by Column')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c934ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f5d103",
   "metadata": {},
   "source": [
    "# Dealing with useless Features and Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51357f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Synonyms','Name','Max Phase'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd046f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['ChEMBL ID','Type'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e642c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.dropna(axis=0,how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17157418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656ed64",
   "metadata": {},
   "source": [
    "# Duplicate Values-Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08373b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c057bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aca029",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f949eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878f452",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e823b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Targets'] = pd.to_numeric(data['Targets'], errors='coerce')\n",
    "data['Bioactivities'] = pd.to_numeric(data['Bioactivities'], errors='coerce')\n",
    "data['AlogP'] = pd.to_numeric(data['AlogP'], errors='coerce')\n",
    "data['Polar Surface Area'] = pd.to_numeric(data['Polar Surface Area'], errors='coerce')\n",
    "data['Inorganic Flag'] = pd.to_numeric(data['Inorganic Flag'], errors='coerce')\n",
    "data['Heavy Atoms'] = pd.to_numeric(data['Heavy Atoms'], errors='coerce')\n",
    "data['HBA (Lipinski)'] = pd.to_numeric(data['HBA (Lipinski)'], errors='coerce')\n",
    "data['HBD (Lipinski)'] = pd.to_numeric(data['HBD (Lipinski)'], errors='coerce')\n",
    "data['#RO5 Violations (Lipinski)'] = pd.to_numeric(data['#RO5 Violations (Lipinski)'], errors='coerce')\n",
    "data['Molecular Weight (Monoisotopic)'] = pd.to_numeric(data['Molecular Weight (Monoisotopic)'], errors='coerce')\n",
    "data['HBA'] = pd.to_numeric(data['HBA'], errors='coerce')\n",
    "data['HBD'] = pd.to_numeric(data['HBD'], errors='coerce')\n",
    "data['#RO5 Violations'] = pd.to_numeric(data['#RO5 Violations'], errors='coerce')\n",
    "data['#Rotatable Bonds'] = pd.to_numeric(data['#Rotatable Bonds'], errors='coerce')\n",
    "data['QED Weighted'] = pd.to_numeric(data['QED Weighted'], errors='coerce')\n",
    "data['CX Acidic pKa'] = pd.to_numeric(data['CX Acidic pKa'], errors='coerce')\n",
    "data['CX Basic pKa'] = pd.to_numeric(data['CX Basic pKa'], errors='coerce')\n",
    "data['CX LogP'] = pd.to_numeric(data['CX LogP'], errors='coerce')\n",
    "data['CX LogD'] = pd.to_numeric(data['CX LogD'], errors='coerce')\n",
    "data['Aromatic Rings'] = pd.to_numeric(data['Aromatic Rings'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df27fe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68932582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a989cec9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2558d73",
   "metadata": {},
   "source": [
    "# Numerical-Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860fb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [col for col in data.columns if data[col].dtype == 'object']\n",
    "num_cols = [col for col in data.columns if data[col].dtype != 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "plotnumber = 1\n",
    "\n",
    "for column in num_cols:\n",
    "    if plotnumber <= 22:\n",
    "        ax = plt.subplot(5, 5, plotnumber)\n",
    "        sns.distplot(data[column])\n",
    "        plt.xlabel(column)\n",
    "        \n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 15))\n",
    "plotnumber = 1\n",
    "\n",
    "for column in ['Passes Ro3', 'Structure Type','Molecular Species']:\n",
    "    if plotnumber <= 11:\n",
    "        ax = plt.subplot(3, 4, plotnumber)\n",
    "        sns.countplot(data[column], palette = 'rocket')\n",
    "        plt.xlabel(column)\n",
    "        \n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef5e45",
   "metadata": {},
   "source": [
    "# Outilers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87efdab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa60f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 20))\n",
    "plotnumber = 1\n",
    "for column in num_cols:\n",
    "    if plotnumber <= 32:\n",
    "       \n",
    "        ax = plt.subplot(7, 5, plotnumber)\n",
    "        sns.boxplot(data[column])\n",
    "        plt.xlabel(column)\n",
    "        \n",
    "    plotnumber += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3671461",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Outliers using IQR\n",
    "def remove_outliers_iqr(data,num_cols, factor=1.5):\n",
    "    for col in num_cols:\n",
    "        q1 = data[col].quantile(0.25)\n",
    "        q3 = data[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_threshold = q1 - factor * iqr\n",
    "        upper_threshold = q3 + factor * iqr\n",
    "        filtered_data = data[(data[col] >= lower_threshold) & (data[col] <= upper_threshold)]\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d7895",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=remove_outliers_iqr(data,num_cols, factor=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab27a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90921e6",
   "metadata": {},
   "source": [
    "# Corrolation based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f55992",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "mask = np.triu(np.ones_like(data.corr()))\n",
    "sns.heatmap(data.corr(), annot=True, linewidths=0.2, mask=mask,cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad8af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using this function we can select correlated features\n",
    "# it will remove the first feature that is correlated with anything other feature\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff399fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_features = correlation(data, 0.9)\n",
    "corr_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf38e1",
   "metadata": {},
   "source": [
    " #both CX LogP and CX LogD are computational methods for predicting the lipophilicity of a compound, \n",
    "#but CX LogD is an extension of CX LogP that takes into account the pH-dependent ionization of the molecule.\n",
    "#CX LogD can be considered more precise than CX LogP because it takes into account the ionization state of the compound at a specific pH,\n",
    "#which can have a significant effect on its lipophilicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00db21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.drop(['#RO5 Violations','CX LogP','HBA','HBD','Molecular Weight (Monoisotopic)',\n",
    "                  'Heavy Atoms'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ffc99b",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe407b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c0997",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Structure Type'] = le.fit_transform(data1['Structure Type'])\n",
    "data1['Molecular Formula'] = le.fit_transform(data1['Molecular Formula'])\n",
    "data1['Molecular Species'] = le.fit_transform(data1['Molecular Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6162585e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=data1.copy().head(5000)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26d851",
   "metadata": {},
   "source": [
    "# ENCODER+DECODER+ MODEL  GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8ce82",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[\"SmileSL\"]=data1[\"Smiles\"]\n",
    "data1.SmileSL=data1.SmileSL.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9865140",
   "metadata": {},
   "outputs": [],
   "source": [
    "deuxletter=set()\n",
    "for c,i in enumerate(data1[\"SmileSL\"]):\n",
    "    le=len(i)-1\n",
    "    for l,j in enumerate(i):\n",
    "        if((j.isalpha())&(j.isupper())&(l<le)):\n",
    "                if((i[l+1].isalpha())&(i[l+1].islower())):\n",
    "                    deuxletter.add(j+i[l+1])\n",
    "        \n",
    "        \n",
    "    \n",
    "                \n",
    "    \n",
    "print(deuxletter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "remp={'Ag':'G', 'Ba':'A', 'Br':'R', 'Ca':'Q', 'Cl':'L', 'Cn':'D', 'Li':'T', 'Mg':'M', 'Na':'E', 'Sc':'X','Sn':'J','Zn':'Z',\"@@\":'V'}\n",
    "for key,rem in remp.items():\n",
    "    data1[\"SmileSL\"]=data1.SmileSL.str.replace(key,rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique=set()\n",
    "maxMol=0\n",
    "for i in data1.SmileSL:\n",
    "    \n",
    "    l=len(i)\n",
    "    if(l>maxMol):\n",
    "        maxMol=l\n",
    "    for j in i:\n",
    "        unique.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "AtomExist={c:i for i,c in enumerate(unique)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cde897",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data1.SmileSL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f3e6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=[]\n",
    "for o,i in enumerate(data1.SmileSL):\n",
    "    c=np.zeros((maxMol,len(AtomExist)))\n",
    "    for j,l in enumerate(i):\n",
    "            c[j,AtomExist[l]]=1\n",
    "    c=c.flatten()\n",
    "    m.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab522ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p=m[0:26240]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4023b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=[]\n",
    "for o,i in enumerate(data1.SmileSL):\n",
    "    c=np.zeros((maxMol,len(AtomExist)))\n",
    "    for j,l in enumerate(i):\n",
    "            c[j,AtomExist[l]]=1\n",
    "    c=c.flatten()\n",
    "    m.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique=set()\n",
    "maxMol=0\n",
    "for i in data1.SmileSL:\n",
    "    \n",
    "    l=len(i)\n",
    "    if(l>maxMol):\n",
    "        maxMol=l\n",
    "    for j in i:\n",
    "        unique.add(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90147cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(): \n",
    "\n",
    "    generator = Sequential()\n",
    "    \n",
    "    generator.add(Dense(256,input_dim=128))\n",
    "    generator.add(LeakyReLU(alpha=0.2))\n",
    "    generator.add(BatchNormalization(momentum=0.8))\n",
    "    generator.add(Dense(256))\n",
    "    generator.add(LeakyReLU(alpha=0.2))\n",
    "    generator.add(BatchNormalization(momentum=0.8))\n",
    "    generator.add(Dense(10755, activation='sigmoid'))\n",
    "    generator.add(Reshape((10755,)))\n",
    "    generator.build((None, 128))\n",
    "    # Conv layer to get to one channe\n",
    "    \n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = build_generator()\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0924eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(): \n",
    "    \n",
    "    \n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(128, input_shape=(10755,)))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(Dense(64))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(Dropout(0.25))\n",
    "    discriminator.add(Dense(64))\n",
    "    discriminator.add(LeakyReLU(alpha=0.2))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    discriminator.build((None, 10755))\n",
    "    return discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889296b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba1ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "rom tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd19488",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(0.0001, 0.5)\n",
    "optimizer1 = Adam(0.00008, 0.5)\n",
    "g_los = BinaryCrossentropy()\n",
    "d_los = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4b9d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BamGAN(Model): \n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "         \n",
    "        \n",
    "    def compile(self, g_los, d_los, *args, **kwargs): \n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        self.d_opt = Adam(0.0002, 0.5)\n",
    "        self.g_opt = Adam(0.0002, 0.5)\n",
    "        self.g_los = g_los\n",
    "        self.d_los = d_los\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        print(batch)\n",
    "        real = batch\n",
    "        fake = self.generator(tf.random.normal((128, 128)), training=False)\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            ypr_real = self.discriminator(real, training=True) \n",
    "            ypr_fake = self.discriminator(fake, training=True)\n",
    "            ypr_all = tf.concat([ypr_real, ypr_fake], axis=0)\n",
    "            y_all = tf.concat([tf.zeros_like(ypr_real), tf.ones_like(ypr_fake)], axis=0) \n",
    "            total_d_los = self.d_los(y_all, ypr_all)\n",
    "        dgrad = d_tape.gradient(total_d_los, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables)) \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            gen_smil = self.generator(tf.random.normal((128,128)), training=True)\n",
    "            predicted_labels = self.discriminator(gen_smil, training=False)\n",
    "            total_g_los = self.g_los(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "        ggrad = g_tape.gradient(total_g_los, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        return {\"d_loss\":total_d_los, \"g_loss\":total_g_los}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "bamgan = BamGAN(generator, discriminator)\n",
    "bamgan.compile( g_los, d_los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8de23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(p).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3077389",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbc327",
   "metadata": {},
   "outputs": [],
   "source": [
    "earl=EarlyStopping(patience=33,monitor=\"g_loss\",mode=\"min\")\n",
    "hist = bamgan.fit( dataset, epochs=1200, callbacks=[earl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57643347",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['d_loss'], label='d_loss')\n",
    "plt.plot(hist.history['g_loss'], label='g_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc7ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=generator.predict(tf.random.normal((128,128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a660c5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(encode):\n",
    "    smiles=\"\"\n",
    "    encode=(encode>0.5).astype(int)\n",
    "    en=encode.reshape(239,45)\n",
    "    for aw,i in enumerate(en):\n",
    "        p=-1\n",
    "        nu=0\n",
    "        for d,j in enumerate(i):\n",
    "            if(j==1):\n",
    "                p=d\n",
    "                nu=nu+1\n",
    "        if nu>1:\n",
    "            break\n",
    "        \n",
    "        if(p!=-1):\n",
    "            for l,k in AtomExist.items():\n",
    "                if(k==p):\n",
    "                    smiles=smiles+l\n",
    "                    break\n",
    "    for key,rem in remp.items():\n",
    "        smiles=smiles.replace(rem,key)\n",
    "    return smiles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(128):\n",
    "    print(decoder(l[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95986d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(40):\n",
    "    print(data1.Smiles.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac40b9b",
   "metadata": {},
   "source": [
    "# Another: codage  + Model LSTM :pour un molecule Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d501bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data1['Smiles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600edfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smile=d.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da78b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(smile))\n",
    "smile.shape      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072c1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Model RNN using SMILES\n",
    "chars=list(d)\n",
    "type(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55764a",
   "metadata": {},
   "source": [
    "# SMILES MAPPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d42af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e23f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled =d.sample(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9602c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=data.copy().head(5000)\n",
    "k=df3['Smiles'].astype('<U53')\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d95f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df_sampled.values\n",
    "trainf=train.astype('<U53')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles(trainf[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989db27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mapping for each char to integer, also mapping for the E (end) is manually inserted into the dictionaries.\n",
    "unique_chars = sorted(list(OrderedDict.fromkeys(chain.from_iterable(trainf))))\n",
    "# maps each unique character as int\n",
    "char_to_int = dict((c, i) for i, c in enumerate(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d37d009",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(unique_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411015fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be5a456",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136113ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add stop letter to dictionary\n",
    "char_to_int.update({\"E\" : len(char_to_int)})\n",
    "int_to_char.update({len(int_to_char) : \"E\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3df879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique characters do we have?\n",
    "mapping_size = len(char_to_int)\n",
    "reverse_mapping_size = len(int_to_char)\n",
    "print (\"Size of the character to integer dictionary is: \", mapping_size)\n",
    "print (\"Size of the integer to character dictionary is: \", reverse_mapping_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ada3357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the datasets\n",
    "def gen_data(data, int_to_char, char_to_int, embed):\n",
    "    \n",
    "    one_hot =  np.zeros((data.shape[0], embed+1, len(char_to_int)),dtype=np.int8)\n",
    "    for i,smile in enumerate(data):\n",
    "        #encode the chars\n",
    "        for j,c in enumerate(smile):\n",
    "            one_hot[i,j,char_to_int[c]] = 1\n",
    "        #Encode endchar\n",
    "        one_hot[i,len(smile):,char_to_int[\"E\"]] = 1\n",
    "    #Return two, one for input and the other for output\n",
    "    return one_hot[:,0:-1,:], one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429a737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get longest sequence\n",
    "embed = max([len(seq) for seq in trainf])\n",
    "\n",
    "# Get datasets\n",
    "X, Y = gen_data(trainf, int_to_char, char_to_int, embed)\n",
    "X, Y = shuffle(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389db9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb4a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de06ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade tensorflow==2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9538cc3",
   "metadata": {},
   "source": [
    "# LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CREATING THE LSTM MODEL \"\"\"\n",
    "\n",
    "# Create the model (simple 2 layer LSTM)\n",
    "model = Sequential()\n",
    "#None accepts any length of features\n",
    "#number of unique features of the input #returns outputs to the hidden layers\n",
    "model.add(LSTM(256, input_shape=(None, X.shape[2]), return_sequences = True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(256, return_sequences = True))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(Y.shape[-1], activation='softmax'))\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cfc827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam')\n",
    "#multi-classification problem\n",
    "# Fit the model\n",
    "history = model.fit(X, Y, epochs = 50, batch_size = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b8de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store to not having to train again...\n",
    "model.save_weights(\"./twolayerlstm\")\n",
    "\n",
    "# Load to continue training or evaluate...\n",
    "model = model.load_weights(\"./twolayerlstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882f8325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Predictions\"\"\"\n",
    "\n",
    "# Calculate predictions\n",
    "predictions = model.predict(X, verbose=0)\n",
    "\n",
    "# Compare to correct result\n",
    "train_res = np.argmax(Y,axis=2)-np.argmax(predictions,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aabaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count correct and incorrect predictions\n",
    "no_false = np.count_nonzero(train_res)\n",
    "no_true = len(Y)*embed-no_false\n",
    "print(\"Average success rate on training set: %s %%\" %str(np.round(100*no_true/(embed*len(Y)),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9d2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the model predictions on the training set next to the true result\n",
    "\n",
    "for i in range(40):\n",
    "    v = model.predict(X[i:i+1]) \n",
    "    idxs = np.argmax(v, axis=2)\n",
    "    pred=  \"\".join([int_to_char[h] for h in idxs[0]])\n",
    "    \n",
    "    \n",
    "    idxs2 = np.argmax(Y[i:i+1], axis=2)\n",
    "    true =  \"\".join([int_to_char[k] for k in idxs2[0]])\n",
    "    if true != pred:\n",
    "        print (true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739af405",
   "metadata": {},
   "source": [
    "# Pretrained Model EXample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65305e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from rdkit import Chem\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n",
    "\n",
    "def generate_smiles(template, num_samples=10):\n",
    "    input = f\"{template} <mask>\"\n",
    "    input_ids = tokenizer.encode(input, return_tensors='pt')\n",
    "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
    "\n",
    "    output = model(input_ids).logits\n",
    "    mask_token_logits = output[0, mask_token_index, :]\n",
    "    top_k_tokens = torch.topk(mask_token_logits, k=num_samples, dim=1).indices[0].tolist()\n",
    "\n",
    "    smiles = []\n",
    "    for token in top_k_tokens:\n",
    "        token_str = tokenizer.decode([token])\n",
    "        smile = input.replace(tokenizer.mask_token, token_str).replace(\" \", \"\")\n",
    "        if is_valid_smiles(smile):\n",
    "            smiles.append(smile)\n",
    "        if len(smiles) == num_samples:\n",
    "            break\n",
    "\n",
    "    return smiles\n",
    "\n",
    "def is_valid_smiles(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# example usage\n",
    "template = \"Cc1ccc(cc1N)C(=O)NCCN\"\n",
    "smiles = generate_smiles(template)\n",
    "print(smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70044247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define a SMILES string to search for\n",
    "smiles = 'Cc1ccc(cc1N)C(=O)NCCNSN'\n",
    "\n",
    "# Send a request to the PubChem API to search for the molecule\n",
    "url = f'https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/smiles/{smiles}/cids/TXT'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the response contains any CIDs (Compound IDs)\n",
    "if response.status_code == 200 and response.text != '':\n",
    "    cids = [int(cid) for cid in response.text.split()]\n",
    "    print(f'The SMILES string \"{smiles}\" matches {len(cids)} known molecules in PubChem.')\n",
    "else:\n",
    "    print(f'The SMILES string \"{smiles}\" does not match any known molecules in PubChem.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = 'Cc1ccc(cc1N)C(=O)NCCNSN'\n",
    "\n",
    "# Send a request to the ChemSpider API to search for the molecule\n",
    "url = f'https://api.rsc.org/compounds/v1/filter/smiles/{smiles}/ids'\n",
    "headers = {'apikey': '<YOUR_CHEMSPIDER_API_KEY>'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the response contains any IDs\n",
    "if response.status_code == 200 and response.json()['count'] > 0:\n",
    "    ids = response.json()['ids']\n",
    "    print(f'The SMILES string \"{smiles}\" matches {len(ids)} known molecules in ChemSpider.')\n",
    "else:\n",
    "    print(f'The SMILES string \"{smiles}\" does not match any known molecules in ChemSpider.')\n",
    "\"Cc1ccc(cc1N)C(=O)NCCN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ed5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
